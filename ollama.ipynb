{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1643d36d-f398-46b5-aca3-6330b22e48a8",
   "metadata": {},
   "source": [
    "# Token Rate Benchmark of DeepSeek-R1:7B with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead1352-bcd6-4c52-b0f7-7a11358c7f96",
   "metadata": {},
   "source": [
    "Here we use [Ollama](https://ollama.com/) to download and serve [DeepSeek-R1:7B](https://ollama.com/library/deepseek-r1:7b). Then, we utilize the [Ollama Python library](https://github.com/ollama/ollama-python)'s API to access [Ollama REST API](https://github.com/ollama/ollama/blob/main/docs/api.md) and calculate the token rate of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081f054-07fa-40f5-888a-ca48ba98f77b",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab21d1-84d6-4fe0-b5cd-e4ee867e67d5",
   "metadata": {},
   "source": [
    "### About Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecdae57-5c41-4b36-b402-7707609a589f",
   "metadata": {},
   "source": [
    "Ollama is a lightweight, extensible framework for building and running Language Models (LM) on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9adaf1-77ea-44fd-82f7-ef223cc22dd1",
   "metadata": {},
   "source": [
    "### About DeepSeek-R1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa08dab-65cf-48aa-93c7-95eb346abadb",
   "metadata": {},
   "source": [
    "DeepSeek-R1 is based on DeepSeek-R1-Zero - a model trained directly via Reinforcement Learning (RL) without Supervised Fine-tuning (SFT). DeepSeek-R1 further improves DeepSeek-R1-Zero by multi-stage training and code-start data before RL.\n",
    "\n",
    "For more information about DeepSeek-R1, check the [original paper](https://arxiv.org/abs/2501.12948)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3aeaff-cffb-4c4f-87c9-04f6ec7984e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              ID              SIZE      MODIFIED    \n",
      "deepseek-r1:7b    0a8c26691023    4.7 GB    3 hours ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07af9113-152b-4e84-b3fc-80f7254f65aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model\n",
      "    architecture        qwen2     \n",
      "    parameters          7.6B      \n",
      "    context length      131072    \n",
      "    embedding length    3584      \n",
      "    quantization        Q4_K_M    \n",
      "\n",
      "  Parameters\n",
      "    stop    \"<｜begin▁of▁sentence｜>\"    \n",
      "    stop    \"<｜end▁of▁sentence｜>\"      \n",
      "    stop    \"<｜User｜>\"                 \n",
      "    stop    \"<｜Assistant｜>\"            \n",
      "\n",
      "  License\n",
      "    MIT License                    \n",
      "    Copyright (c) 2023 DeepSeek    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ollama show deepseek-r1:7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2d1fa-1ca6-4c86-99b4-d3c71c1647f9",
   "metadata": {},
   "source": [
    "We can see that DeepSeek-R1:7B is a distilled version using the [Qwen2 LLM architecture](https://qwenlm.github.io/blog/qwen2/). It is quantized to 4 bits per weight parameter. With 7.6B parameters, that means the model size will be approximately 7.6 * 10^9 * 4 / 8 / 10^9 = 3.8 GB. There might be increased components for pre/post-processing that lead to a final 4.7 GB memory consumption on NVME storage. Besides, the models need to hold KV caches and other intermediate data during executions, so the GPU VRAM consumption will be even higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003973d-dd32-42a4-bb25-3d582218c4d0",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c788ae-378d-4c19-8be6-23d70ac3c079",
   "metadata": {},
   "source": [
    "1. Python libraries: `ollama`\n",
    "2. Ollama CLI\n",
    "3. DeepSeek-R1:7B installed via Ollama CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd5cc8-2084-4af8-9c36-07aa0c5bf157",
   "metadata": {},
   "source": [
    "## Running via Ollama CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e7a3b-54ea-4db1-b90e-f8c898a01866",
   "metadata": {},
   "source": [
    "Run this command: `ollama run deepseek-r1:7b --verbose \"<YOUR PROMPT>\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4295b99-b640-4d52-981b-9efcb5d6a4a1",
   "metadata": {},
   "source": [
    "Below shows a sample output from a CLI console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580ea3b-9b6c-4b25-b528-a6dcb0a92272",
   "metadata": {},
   "source": [
    "```bash\n",
    "(edge-ai) jetty@ubuntu:~$ ollama run deepseek-r1:7b --verbose \"What is Theory of Mind (ToM)? Give a simple example about it. Also, How do you think an LLM can somehow achieve ToM?\"\n",
    "```\n",
    "<think>\n",
    "Okay, so I need to figure out what Theory of Mind (ToM) is and come up with a simple example. Then, I also have to think about how an LLM might achieve this ability.\n",
    "\n",
    "First, I remember from psychology that ToM has something to do with understanding other people's thoughts and emotions. But I'm not entirely sure about the details. Maybe it's like when we think about what others are thinking or\n",
    "feeling? So, if my friend is upset because they lost their keys, I might try to figure out why they're upset instead of just focusing on losing the keys themselves.\n",
    "\n",
    "Wait, the user gave an example involving an LLM and a child who misunderstands sarcasm. That's a good simple example. Maybe I can use that or come up with another one. Let me think: perhaps something like when someone is trying to\n",
    "explain their decision to a friend, but the friend doesn't get it because they're using different assumptions.\n",
    "\n",
    "Now, about how an LLM achieves ToM. Well, I know that LLMs process information based on patterns and data they've been trained on. So maybe they can infer mental states by analyzing the context of conversations or texts. But does\n",
    "this mean they truly understand others' thoughts or is it just surface-level analysis?\n",
    "\n",
    "I'm not sure if an LLM actually has a deeper understanding, but it can simulate ToM by interpreting the intent behind messages and responding appropriately. For example, in sarcasm detection, which relates to ToM.\n",
    "\n",
    "Putting it all together: ToM is about understanding others' mental states, like thoughts and feelings. An LLM achieves this through analyzing context and inferring intent rather than having actual consciousness of other minds.\n",
    "</think>\n",
    "\n",
    "**Theory of Mind (ToM):** It is the ability to comprehend the mental states—such as thoughts, beliefs, intentions, emotions, etc.—of oneself and others. Essentially, it's the capacity to understand that other people have their own\n",
    "perspectives, motivations, and mental processes.\n",
    "\n",
    "**Simple Example:** Imagine a child receiving a message from an adult saying, \"I told you I was busy.\" The child might interpret this as the adult being uninterested because they were indeed busy. This interpretation reflects a\n",
    "basic understanding of sarcasm, which is a form of ToM.\n",
    "\n",
    "**How LLMs Achieve ToM:**\n",
    "Language models like ChatGPT achieve ToM by analyzing context and inferring intent from interactions. They process input text to understand the user's intentions, emotions, or background knowledge implicitly. While they don't\n",
    "possess consciousness or true understanding, their responses are guided by patterns learned during training, effectively simulating ToM through contextual analysis.\n",
    "\n",
    "In summary, ToM involves interpreting others' mental states, and LLMs achieve this by analyzing context and inferring intent without actual consciousness of other minds.\n",
    "\n",
    "```bash\n",
    "total duration:       1m28.165662824s\n",
    "load duration:        48.590339ms\n",
    "prompt eval count:    34 token(s)\n",
    "prompt eval duration: 185.009901ms\n",
    "prompt eval rate:     183.77 tokens/s\n",
    "eval count:           575 token(s)\n",
    "eval duration:        1m27.930198598s\n",
    "eval rate:            6.54 tokens/s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1742b-8c31-4f63-9cfe-0ea4c7e9dcb7",
   "metadata": {},
   "source": [
    "The output is streamed to the console. We can see that reasoning LLMs like DeepSeek-R1 first performs a thinking phase within the `<think></think>` block, then provides the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63156f-ef56-43d0-b95e-2a091a9310b0",
   "metadata": {},
   "source": [
    "From the `--verbose` flag, Ollama CLI directly provides us with the token rate of the current inference. We can see that the token rate at the prefill phase is measured in `prompt eval rate` = 183 tokens/sec, while the token rate at the decoding phase is measured in `eval rate` = 6.5 tokens/sec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd9b280-cfbf-4432-a2e6-a6dfb1a279ad",
   "metadata": {},
   "source": [
    "By running `sudo tegrastats` at the same time on Jetson Orin Nano Super, we can see that the GPU VRAM consumption during inference is around 6.6 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85218f9d-7119-4709-88b2-af1a582f31d5",
   "metadata": {},
   "source": [
    "```bash\n",
    "03-23-2025 17:58:38 RAM 6595/7620MB (lfb 4x1MB) SWAP 107/3810MB (cached 0MB) CPU [0%@729,0%@729,0%@729,0%@729,0%@729,0%@729] EMC_FREQ 0%@2133 GR3D_FREQ 0%@[305] NVDEC off NVJPG off NVJPG1 off VIC off OFA off APE 200 cpu@52.343C soc2@51.5C soc0@50.875C gpu@52.531C tj@52.531C soc1@52.25C VDD_IN 4794mW/9018mW VDD_CPU_GPU_CV 466mW/2699mW VDD_SOC 1478mW/2177mW\n",
    "03-23-2025 17:58:39 RAM 6596/7620MB (lfb 4x1MB) SWAP 107/3810MB (cached 0MB) CPU [0%@729,1%@729,0%@729,0%@729,0%@729,0%@729] EMC_FREQ 0%@2133 GR3D_FREQ 0%@[305] NVDEC off NVJPG off NVJPG1 off VIC off OFA off APE 200 cpu@52.593C soc2@51.406C soc0@50.843C gpu@52.5C tj@52.593C soc1@52.156C VDD_IN 4794mW/8989mW VDD_CPU_GPU_CV 466mW/2684mW VDD_SOC 1478mW/2172mW\n",
    "03-23-2025 17:58:40 RAM 6596/7620MB (lfb 4x1MB) SWAP 107/3810MB (cached 0MB) CPU [0%@729,0%@729,0%@729,0%@729,0%@729,0%@729] EMC_FREQ 0%@2133 GR3D_FREQ 0%@[305] NVDEC off NVJPG off NVJPG1 off VIC off OFA off APE 200 cpu@52.312C soc2@51.406C soc0@50.625C gpu@52.375C tj@52.687C soc1@52.125C VDD_IN 4755mW/8961mW VDD_CPU_GPU_CV 466mW/2669mW VDD_SOC 1478mW/2167mW\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385d560-268d-468b-b986-b3e05112769e",
   "metadata": {},
   "source": [
    "## Running from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb47674-cfe9-47d6-8217-3a33bcfdb9b0",
   "metadata": {},
   "source": [
    "First, make sure `http_proxy` is unset for the current session, as suggested in the [official FAQ](# https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-use-ollama-behind-a-proxy).\n",
    "\n",
    "> Avoid setting HTTP_PROXY. Ollama does not use HTTP for model pulls, only HTTPS. Setting HTTP_PROXY may interrupt client connections to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da787c55-ca8f-4e01-a367-9596b3c2b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export http_proxy=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929f17ba-83d5-4ef4-8020-f4dd2cdaf2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = 'What is Theory of Mind (ToM)? Give a simple example about it. Also, How do you think an LLM can somehow achieve ToM?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437797e0-e0d1-404f-81e6-50a6cf66ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e3154c-7024-45a5-afaf-215d4345b961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out what Theory of Mind (ToM) is and come up with a simple example. Then, I also have to think about how an LLM might achieve ToM. Hmm, let me start by understanding what ToM really means.\n",
      "\n",
      "From what I remember, Theory of Mind refers to the ability to understand that other people have their own thoughts, beliefs, intentions, goals, and perspectives different from one's own. It's like being able to put yourself in someone else's shoes. So, for example, if my friend is upset because they can't play outside, I might think about why they're upset—maybe they don't want to get wet or maybe something happened that made them sad.\n",
      "\n",
      "Now, an LLM has to achieve this ability somehow. I'm a bit fuzzy on how exactly. Maybe it's through training data? Like if the model is exposed to a lot of conversations where people explain their thoughts and feelings, the model might learn patterns associated with ToM. Or perhaps it's more about generating responses that are coherent from an empathetic perspective.\n",
      "\n",
      "Wait, but can an LLM truly understand others' mental states? I mean, it doesn't have consciousness or anything, so maybe it just models them statistically. So when I ask an LLM to explain something, it has to infer the user's thoughts based on the context and previous interactions, which might be a part of achieving ToM.\n",
      "\n",
      "Also, how does this relate to language modeling tasks? Maybe in generating responses that are empathetic or by understanding sarcasm or idioms, which require some level of understanding beyond surface-level words. So perhaps the model uses its extensive training data on human conversations to infer ToM indirectly.\n",
      "\n",
      "But I'm not entirely sure if LLMs actually have ToM as humans do, or if they just simulate it through patterns and context. It's possible that their ability is limited compared to human ToM because they lack true understanding but can mimic some aspects through statistical learning.\n",
      "\n",
      "So putting this together: ToM is the capacity to understand others' mental states, like thinking about someone else's perspective. An LLM might achieve something akin to ToM by being trained on diverse conversations that require interpreting others' emotions and intentions, allowing it to generate responses that seem empathetic or to infer higher-order thoughts from context.\n",
      "</think>\n",
      "\n",
      "**Theory of Mind (ToM): Understanding Others' Mental States**\n",
      "\n",
      "Theory of Mind (ToM) is the ability to comprehend that individuals possess their own thoughts, beliefs, and perspectives distinct from one's own. It involves understanding others' mental states, such as emotions, intentions, and goals. For example, when a friend expresses sadness over losing their pet, we might infer their emotional state or the underlying reason for their sorrow.\n",
      "\n",
      "**How an LLM Achieves ToM:**\n",
      "\n",
      "An AI Large Language Model (LLM) achieves something akin to Theory of Mind through extensive training on diverse conversations that include discussions about emotions and perspectives. By processing numerous interactions where individuals explain their thoughts, the model learns statistical patterns associated with understanding others' mental states. This enables it to generate empathetic responses or infer higher-order thoughts from context, effectively simulating ToM without conscious understanding. However, LLMs do not possess true consciousness or understanding; they merely mimic aspects of ToM through pattern recognition and contextual awareness in their training data.\n",
      "\n",
      "In summary, while LLMs lack true human-like ToM due to the absence of consciousness, they can simulate it by analyzing and interpreting vast amounts of conversational data, allowing them to respond empathetically and infer mental states based on context."
     ]
    }
   ],
   "source": [
    "stream = chat(\n",
    "    model='deepseek-r1:7b',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': default_prompt,\n",
    "    }],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "metrics = None\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)\n",
    "    if chunk['done']:\n",
    "        metrics = chunk.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433776d-9a29-46de-8f55-742f6c015f19",
   "metadata": {},
   "source": [
    "According to the [official documentation](https://github.com/ollama/ollama/blob/main/docs/api.md#response-10), the last response will contain some evaluation metrics, so we save the final metrics for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3566b263-3c81-48fb-a5c9-f3df3a5d02ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'deepseek-r1:7b',\n",
       " 'created_at': '2025-03-23T11:47:43.742901832Z',\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'total_duration': 96964881225,\n",
       " 'load_duration': 46586728,\n",
       " 'prompt_eval_count': 34,\n",
       " 'prompt_eval_duration': 20936091,\n",
       " 'eval_count': 734,\n",
       " 'eval_duration': 96895594971,\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': '',\n",
       "  'images': None,\n",
       "  'tool_calls': None}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ef544-99b6-41ea-886c-25e2727801fc",
   "metadata": {},
   "source": [
    "According to the [official documentation](https://github.com/ollama/ollama/blob/main/docs/api.md#durations):\n",
    "> All durations are returned in nanoseconds.\n",
    "\n",
    "So we need to transform the data to a similar format of the one we see in the CLI console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "397966fb-8e15-425b-a551-0c7ffe907dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total duration:\t\t96.9649 sec\n",
      "load duration:\t\t46.5867 ms\n",
      "prompt eval count:\t34 token(s)\n",
      "prompt eval duration:\t20.9361 ms\n",
      "prompt eval rate:\t1623.99 tokens/s\n",
      "eval count:\t\t734 token(s)\n",
      "eval duration:\t\t96.8956 ms\n",
      "eval rate:\t\t7.58 tokens/s\n"
     ]
    }
   ],
   "source": [
    "print('total duration:\\t\\t%.4f sec' % (metrics['total_duration'] / 1e9))\n",
    "print('load duration:\\t\\t%.4f ms' % (metrics['load_duration'] / 1e6))\n",
    "print('prompt eval count:\\t%d token(s)' % (metrics['prompt_eval_count']))\n",
    "print('prompt eval duration:\\t%.4f ms' % (metrics['prompt_eval_duration'] / 1e6))\n",
    "print('prompt eval rate:\\t%.2f tokens/s' % (metrics['prompt_eval_count'] / metrics['prompt_eval_duration'] * 1e9))\n",
    "print('eval count:\\t\\t%d token(s)' % (metrics['eval_count']))\n",
    "print('eval duration:\\t\\t%.4f ms' % (metrics['eval_duration'] / 1e9))\n",
    "print('eval rate:\\t\\t%.2f tokens/s' % (metrics['eval_count'] / metrics['eval_duration'] * 1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9666f97-8f30-474b-a823-ef79cd342288",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d03a52-88e5-45df-9ba2-cab99316bcd1",
   "metadata": {},
   "source": [
    "1. [Open WebUI](https://www.jetson-ai-lab.com/tutorial_openwebui.html): [Open WebUI](https://github.com/open-webui/open-webui) is an AI platform designed to operate entirely offline. More specifically, it is a versatile, **brower-based** interface for running and managing LLMs locally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Edge AI",
   "language": "python",
   "name": "edge-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
